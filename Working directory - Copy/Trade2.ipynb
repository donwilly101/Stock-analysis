{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2894e782-fbbd-4766-a917-7f82cd3a90f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758cda08-f69f-47ba-89d1-d058a46634c4",
   "metadata": {},
   "source": [
    "### MLflow â€“ Experiment Tracking & Model Versioning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f637c363-7102-440a-b4bc-84623359e027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlflow in c:\\users\\user\\anaconda3\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: mlflow-skinny==3.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow) (3.6.0)\n",
      "Requirement already satisfied: mlflow-tracing==3.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow) (3.6.0)\n",
      "Requirement already satisfied: Flask-CORS<7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow) (6.0.1)\n",
      "Requirement already satisfied: Flask<4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow) (3.1.0)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow) (1.17.2)\n",
      "Requirement already satisfied: cryptography<47,>=43.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow) (44.0.1)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow) (7.1.0)\n",
      "Requirement already satisfied: graphene<4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow) (3.4.3)\n",
      "Requirement already satisfied: huey<3,>=2.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow) (2.5.4)\n",
      "Requirement already satisfied: matplotlib<4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow) (3.10.0)\n",
      "Requirement already satisfied: numpy<3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow) (2.1.3)\n",
      "Requirement already satisfied: pandas<3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow) (2.2.3)\n",
      "Requirement already satisfied: pyarrow<23,>=4.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow) (19.0.0)\n",
      "Requirement already satisfied: scikit-learn<2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow) (1.6.1)\n",
      "Requirement already satisfied: scipy<2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow) (1.15.3)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow) (2.0.39)\n",
      "Requirement already satisfied: waitress<4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow) (3.0.2)\n",
      "Requirement already satisfied: cachetools<7,>=5.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.6.0->mlflow) (5.5.1)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.6.0->mlflow) (8.1.8)\n",
      "Requirement already satisfied: cloudpickle<4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.6.0->mlflow) (3.0.0)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.6.0->mlflow) (0.73.0)\n",
      "Requirement already satisfied: fastapi<1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.6.0->mlflow) (0.122.0)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.6.0->mlflow) (3.1.43)\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.6.0->mlflow) (8.5.0)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.6.0->mlflow) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-proto<3,>=1.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.6.0->mlflow) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.6.0->mlflow) (1.38.0)\n",
      "Requirement already satisfied: packaging<26 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.6.0->mlflow) (24.2)\n",
      "Requirement already satisfied: protobuf<7,>=3.12.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.6.0->mlflow) (6.33.1)\n",
      "Requirement already satisfied: pydantic<3,>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.6.0->mlflow) (2.10.3)\n",
      "Requirement already satisfied: python-dotenv<2,>=0.19.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.6.0->mlflow) (1.1.0)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.6.0->mlflow) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.6.0->mlflow) (2.32.3)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.6.0->mlflow) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.6.0->mlflow) (4.12.2)\n",
      "Requirement already satisfied: uvicorn<1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.6.0->mlflow) (0.38.0)\n",
      "Requirement already satisfied: Mako in c:\\users\\user\\anaconda3\\lib\\site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.10)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from click<9,>=7.0->mlflow-skinny==3.6.0->mlflow) (0.4.6)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\user\\anaconda3\\lib\\site-packages (from cryptography<47,>=43.0.0->mlflow) (1.17.1)\n",
      "Requirement already satisfied: google-auth~=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.6.0->mlflow) (2.43.0)\n",
      "Requirement already satisfied: pywin32>=304 in c:\\users\\user\\anaconda3\\lib\\site-packages (from docker<8,>=4.0.0->mlflow) (308)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from docker<8,>=4.0.0->mlflow) (2.3.0)\n",
      "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from fastapi<1->mlflow-skinny==3.6.0->mlflow) (0.50.0)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from fastapi<1->mlflow-skinny==3.6.0->mlflow) (0.0.4)\n",
      "Requirement already satisfied: Werkzeug>=3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from Flask<4->mlflow) (3.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from Flask<4->mlflow) (3.1.6)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from Flask<4->mlflow) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from Flask<4->mlflow) (1.9.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.6.0->mlflow) (4.0.7)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.6.0->mlflow) (4.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.6.0->mlflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.6.0->mlflow) (4.9.1)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from graphene<4->mlflow) (3.2.7)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from graphene<4->mlflow) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\user\\anaconda3\\lib\\site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.6.0->mlflow) (3.21.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow) (3.2.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.59b0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.6.0->mlflow) (0.59b0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas<3->mlflow) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas<3->mlflow) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.6.0->mlflow) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.6.0->mlflow) (2.27.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.6.0->mlflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.6.0->mlflow) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.6.0->mlflow) (2025.8.3)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.6.0->mlflow) (0.4.8)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn<2->mlflow) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn<2->mlflow) (3.5.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from starlette<0.51.0,>=0.40.0->fastapi<1->mlflow-skinny==3.6.0->mlflow) (4.7.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.51.0,>=0.40.0->fastapi<1->mlflow-skinny==3.6.0->mlflow) (1.3.0)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from uvicorn<1->mlflow-skinny==3.6.0->mlflow) (0.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography<47,>=43.0.0->mlflow) (2.21)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from Jinja2>=3.1.2->Flask<4->mlflow) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install mlflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e9f403-fb4e-4a63-bae8-e68a7bc5a4ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\user\n",
      "Folders: ['.anaconda', '.cache', '.codeium', '.conda', '.condarc', '.continuum', '.EasyOCR', '.ipynb_checkpoints', '.ipython', '.jupyter', '.keras', '.matplotlib', '.spyder-py3', '.virtual_documents', '.vscode', '3D Objects', 'anaconda3', 'AppData', 'Application Data', 'artifacts', 'bird (1).jpg', 'Capstone Project EDA.1.ipynb', 'Car Insurance.csv', 'Case study - Unsupervised Learning (PCA).ipynb', 'cat.jpg', 'Class work Numpy & Pandas - Completed (1).ipynb', 'Comp Vision.ipynb', 'Contacts', 'Cookies', 'Dataset_ecommerce.csv', 'Desktop', 'Documents', 'Downloads', 'edb_mtk.exe', 'edb_npgsql.exe', 'edb_pem_agent.exe-20250628025359', 'edb_pem_server.exe-20250628025519', 'edb_pgagent_pg17.exe', 'edb_pgbouncer.exe', 'edb_pgjdbc.exe', 'edb_psqlodbc.exe', 'edb_psqlodbc.exe-20250628025735', 'edb_sqlprofiler_pg17.exe', 'edb_xdb_62.exe', 'edb_xdb_7.exe', 'Favorites', 'Hello World.ipynb', 'IntelGraphicsProfiles', 'Introduction to python .ipynb', 'Links', 'Local Settings', 'logs.log', 'Machine LEarning.ipynb', 'Microsoft', 'MicrosoftEdgeBackups', 'Music', 'My Documents', 'NCH Software Suite', 'NetHood', 'NTUSER.DAT', 'ntuser.dat.LOG1', 'ntuser.dat.LOG2', 'NTUSER.DAT{2ad838bc-efea-11ee-a54d-000d3a94eaa1}.TM.blf', 'NTUSER.DAT{2ad838bc-efea-11ee-a54d-000d3a94eaa1}.TMContainer00000000000000000001.regtrans-ms', 'NTUSER.DAT{2ad838bc-efea-11ee-a54d-000d3a94eaa1}.TMContainer00000000000000000002.regtrans-ms', 'ntuser.ini', 'Numpy & Pandas Live Class.ipynb', 'Numpy.ipynb', 'OneDrive', 'OneDrive - University of Salford', 'Pandas.ipynb', 'PCA - Case study - Unsupervised Learning (PCA).ipynb', 'pemhttpd.exe', 'Pictures', 'postgis_3_5_pg17.exe', 'postgresql_17.exe', 'PrintHood', 'priory_elmswood', 'priory_elmswood.csv', 'Putting Datsets together.ipynb', 'Putting everything together 2.ipynb', 'Pyhton Data Types.ipynb', 'Quick Basket Checkout App.ipynb', 'receipt.png', 'receipt_with_date.png', 'Recent', 'retailmart_data.csv', 'sales_data.csv', 'Saved Games', 'Searches', 'SendTo', 'simple_receipt.png', 'Social_Network_Ads.csv', 'Start Menu', 'Supervised Class Work PCA .ipynb', 'Supreme Retails.ipynb', 'Templates', 'Times Series.ipynb', 'Trade2.ipynb', 'Untitled Folder', 'Untitled Folder 1', 'Untitled.ipynb', 'Untitled1.ipynb', 'Untitled10.ipynb', 'Untitled16.ipynb', 'Untitled2.ipynb', 'Untitled3.ipynb', 'Untitled9.ipynb', 'Videos', 'Viz.ipynb', 'Week 2.ipynb', 'Williams_Ajose_df.html', 'Williams_Ajose_df.ipynb', 'Workshop 5', 'WORKSHOP1.ipynb', 'Workshop5']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Folders:\", os.listdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58e0d3b-58b9-4e32-a8f2-d4dfa17071a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\anaconda3\\envs\\easyocr_env\\Lib\\site-packages\\numpy\\_core\\tests\\data\\astype_copy.pkl\n",
      ".\\anaconda3\\envs\\Pycaret\\Lib\\site-packages\\examples\\global_fc\\X.pkl\n",
      ".\\anaconda3\\envs\\Pycaret\\Lib\\site-packages\\examples\\global_fc\\y.pkl\n",
      ".\\anaconda3\\envs\\Pycaret\\Lib\\site-packages\\joblib\\test\\data\\joblib_0.10.0_pickle_py27_np17.pkl\n",
      ".\\anaconda3\\envs\\Pycaret\\Lib\\site-packages\\joblib\\test\\data\\joblib_0.10.0_pickle_py33_np18.pkl\n",
      ".\\anaconda3\\envs\\Pycaret\\Lib\\site-packages\\joblib\\test\\data\\joblib_0.10.0_pickle_py34_np19.pkl\n",
      ".\\anaconda3\\envs\\Pycaret\\Lib\\site-packages\\joblib\\test\\data\\joblib_0.10.0_pickle_py35_np19.pkl\n",
      ".\\anaconda3\\envs\\Pycaret\\Lib\\site-packages\\joblib\\test\\data\\joblib_0.11.0_pickle_py36_np111.pkl\n",
      ".\\anaconda3\\envs\\Pycaret\\Lib\\site-packages\\joblib\\test\\data\\joblib_0.9.2_pickle_py27_np16.pkl\n",
      ".\\anaconda3\\envs\\Pycaret\\Lib\\site-packages\\joblib\\test\\data\\joblib_0.9.2_pickle_py27_np17.pkl\n",
      ".\\anaconda3\\envs\\Pycaret\\Lib\\site-packages\\joblib\\test\\data\\joblib_0.9.2_pickle_py33_np18.pkl\n",
      ".\\anaconda3\\envs\\Pycaret\\Lib\\site-packages\\joblib\\test\\data\\joblib_0.9.2_pickle_py34_np19.pkl\n",
      ".\\anaconda3\\envs\\Pycaret\\Lib\\site-packages\\joblib\\test\\data\\joblib_0.9.2_pickle_py35_np19.pkl\n",
      ".\\anaconda3\\envs\\Pycaret\\Lib\\site-packages\\numpy\\core\\tests\\data\\astype_copy.pkl\n",
      ".\\anaconda3\\envs\\Pycaret\\Lib\\site-packages\\numpy\\core\\tests\\data\\numpy_2_0_array.pkl\n",
      ".\\anaconda3\\envs\\Pycaret\\Lib\\site-packages\\pandas\\tests\\io\\data\\legacy_pickle\\1.2.4\\empty_frame_v1_2_4-GH#42345.pkl\n",
      ".\\anaconda3\\envs\\Pycaret\\Lib\\site-packages\\pandas\\tests\\io\\data\\pickle\\test_mi_py27.pkl\n",
      ".\\anaconda3\\envs\\Pycaret\\Lib\\site-packages\\pandas\\tests\\io\\data\\pickle\\test_py27.pkl\n",
      ".\\anaconda3\\envs\\Pycaret\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\tests\\results\\sm-0.9-sarimax.pkl\n",
      ".\\anaconda3\\Lib\\site-packages\\joblib\\test\\data\\joblib_0.10.0_pickle_py27_np17.pkl\n",
      ".\\anaconda3\\Lib\\site-packages\\joblib\\test\\data\\joblib_0.10.0_pickle_py33_np18.pkl\n",
      ".\\anaconda3\\Lib\\site-packages\\joblib\\test\\data\\joblib_0.10.0_pickle_py34_np19.pkl\n",
      ".\\anaconda3\\Lib\\site-packages\\joblib\\test\\data\\joblib_0.10.0_pickle_py35_np19.pkl\n",
      ".\\anaconda3\\Lib\\site-packages\\joblib\\test\\data\\joblib_0.11.0_pickle_py36_np111.pkl\n",
      ".\\anaconda3\\Lib\\site-packages\\joblib\\test\\data\\joblib_0.9.2_pickle_py27_np16.pkl\n",
      ".\\anaconda3\\Lib\\site-packages\\joblib\\test\\data\\joblib_0.9.2_pickle_py27_np17.pkl\n",
      ".\\anaconda3\\Lib\\site-packages\\joblib\\test\\data\\joblib_0.9.2_pickle_py33_np18.pkl\n",
      ".\\anaconda3\\Lib\\site-packages\\joblib\\test\\data\\joblib_0.9.2_pickle_py34_np19.pkl\n",
      ".\\anaconda3\\Lib\\site-packages\\joblib\\test\\data\\joblib_0.9.2_pickle_py35_np19.pkl\n",
      ".\\anaconda3\\Lib\\site-packages\\numpy\\_core\\tests\\data\\astype_copy.pkl\n",
      ".\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\tests\\results\\sm-0.9-sarimax.pkl\n",
      ".\\anaconda3\\pkgs\\joblib-1.3.2-pyhd8ed1ab_0\\site-packages\\joblib\\test\\data\\joblib_0.10.0_pickle_py27_np17.pkl\n",
      ".\\anaconda3\\pkgs\\joblib-1.3.2-pyhd8ed1ab_0\\site-packages\\joblib\\test\\data\\joblib_0.10.0_pickle_py33_np18.pkl\n",
      ".\\anaconda3\\pkgs\\joblib-1.3.2-pyhd8ed1ab_0\\site-packages\\joblib\\test\\data\\joblib_0.10.0_pickle_py34_np19.pkl\n",
      ".\\anaconda3\\pkgs\\joblib-1.3.2-pyhd8ed1ab_0\\site-packages\\joblib\\test\\data\\joblib_0.10.0_pickle_py35_np19.pkl\n",
      ".\\anaconda3\\pkgs\\joblib-1.3.2-pyhd8ed1ab_0\\site-packages\\joblib\\test\\data\\joblib_0.11.0_pickle_py36_np111.pkl\n",
      ".\\anaconda3\\pkgs\\joblib-1.3.2-pyhd8ed1ab_0\\site-packages\\joblib\\test\\data\\joblib_0.9.2_pickle_py27_np16.pkl\n",
      ".\\anaconda3\\pkgs\\joblib-1.3.2-pyhd8ed1ab_0\\site-packages\\joblib\\test\\data\\joblib_0.9.2_pickle_py27_np17.pkl\n",
      ".\\anaconda3\\pkgs\\joblib-1.3.2-pyhd8ed1ab_0\\site-packages\\joblib\\test\\data\\joblib_0.9.2_pickle_py33_np18.pkl\n",
      ".\\anaconda3\\pkgs\\joblib-1.3.2-pyhd8ed1ab_0\\site-packages\\joblib\\test\\data\\joblib_0.9.2_pickle_py34_np19.pkl\n",
      ".\\anaconda3\\pkgs\\joblib-1.3.2-pyhd8ed1ab_0\\site-packages\\joblib\\test\\data\\joblib_0.9.2_pickle_py35_np19.pkl\n",
      ".\\anaconda3\\pkgs\\joblib-1.4.2-py313haa95532_0\\Lib\\site-packages\\joblib\\test\\data\\joblib_0.10.0_pickle_py27_np17.pkl\n",
      ".\\anaconda3\\pkgs\\joblib-1.4.2-py313haa95532_0\\Lib\\site-packages\\joblib\\test\\data\\joblib_0.10.0_pickle_py33_np18.pkl\n",
      ".\\anaconda3\\pkgs\\joblib-1.4.2-py313haa95532_0\\Lib\\site-packages\\joblib\\test\\data\\joblib_0.10.0_pickle_py34_np19.pkl\n",
      ".\\anaconda3\\pkgs\\joblib-1.4.2-py313haa95532_0\\Lib\\site-packages\\joblib\\test\\data\\joblib_0.10.0_pickle_py35_np19.pkl\n",
      ".\\anaconda3\\pkgs\\joblib-1.4.2-py313haa95532_0\\Lib\\site-packages\\joblib\\test\\data\\joblib_0.11.0_pickle_py36_np111.pkl\n",
      ".\\anaconda3\\pkgs\\joblib-1.4.2-py313haa95532_0\\Lib\\site-packages\\joblib\\test\\data\\joblib_0.9.2_pickle_py27_np16.pkl\n",
      ".\\anaconda3\\pkgs\\joblib-1.4.2-py313haa95532_0\\Lib\\site-packages\\joblib\\test\\data\\joblib_0.9.2_pickle_py27_np17.pkl\n",
      ".\\anaconda3\\pkgs\\joblib-1.4.2-py313haa95532_0\\Lib\\site-packages\\joblib\\test\\data\\joblib_0.9.2_pickle_py33_np18.pkl\n",
      ".\\anaconda3\\pkgs\\joblib-1.4.2-py313haa95532_0\\Lib\\site-packages\\joblib\\test\\data\\joblib_0.9.2_pickle_py34_np19.pkl\n",
      ".\\anaconda3\\pkgs\\joblib-1.4.2-py313haa95532_0\\Lib\\site-packages\\joblib\\test\\data\\joblib_0.9.2_pickle_py35_np19.pkl\n",
      ".\\anaconda3\\pkgs\\numpy-1.26.4-py310hf667824_0\\Lib\\site-packages\\numpy\\core\\tests\\data\\astype_copy.pkl\n",
      ".\\anaconda3\\pkgs\\numpy-1.26.4-py310hf667824_0\\Lib\\site-packages\\numpy\\core\\tests\\data\\numpy_2_0_array.pkl\n",
      ".\\anaconda3\\pkgs\\numpy-base-2.1.3-py313h6011491_0\\Lib\\site-packages\\numpy\\_core\\tests\\data\\astype_copy.pkl\n",
      ".\\anaconda3\\pkgs\\sktime-0.26.0-py310h5588dad_0\\Lib\\site-packages\\examples\\global_fc\\X.pkl\n",
      ".\\anaconda3\\pkgs\\sktime-0.26.0-py310h5588dad_0\\Lib\\site-packages\\examples\\global_fc\\y.pkl\n",
      ".\\anaconda3\\pkgs\\statsmodels-0.14.4-py313h827c3e9_0\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\tests\\results\\sm-0.9-sarimax.pkl\n",
      ".\\anaconda3\\pkgs\\statsmodels-0.14.5-py310h8f3aa81_0\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\tests\\results\\sm-0.9-sarimax.pkl\n",
      ".\\AppData\\Local\\Jedi\\Jedi\\CPython-313-33\\4305da1ea25c27fce08bd14001b76fd54fe42a0724bbd5168c76680a56eda5be-15ffbaf80947ac30c451e7644c2bec83dde39cac6968ead42084f26d7aca131b.pkl\n",
      ".\\AppData\\Local\\Jedi\\Jedi\\CPython-313-33\\4305da1ea25c27fce08bd14001b76fd54fe42a0724bbd5168c76680a56eda5be-284f350b8753c9700f969a0a4dd2ac5f7a46ae8b41f341898907b08e6ad37155.pkl\n",
      ".\\AppData\\Local\\Jedi\\Jedi\\CPython-313-33\\4305da1ea25c27fce08bd14001b76fd54fe42a0724bbd5168c76680a56eda5be-355bfb2b3d42ab87e7513d51b7dc9dd9d0b6d0fb25fc89c3529d8f2c4ff610e9.pkl\n",
      ".\\AppData\\Local\\Jedi\\Jedi\\CPython-313-33\\4305da1ea25c27fce08bd14001b76fd54fe42a0724bbd5168c76680a56eda5be-4fe531273c7b040602a865eb85f8b01d58afae71504f62305ac624c14b4700a7.pkl\n",
      ".\\AppData\\Local\\Jedi\\Jedi\\CPython-313-33\\4305da1ea25c27fce08bd14001b76fd54fe42a0724bbd5168c76680a56eda5be-66be76dfdb093e6bc6b22b43e5287f86e8f7c063b395474c12b885bb3e446b55.pkl\n",
      ".\\AppData\\Local\\Jedi\\Jedi\\CPython-313-33\\4305da1ea25c27fce08bd14001b76fd54fe42a0724bbd5168c76680a56eda5be-689615bd49e633e693a19291b9d50cba9cfe1300eb17c205f7b8872f4ba834b6.pkl\n",
      ".\\AppData\\Local\\Jedi\\Jedi\\CPython-313-33\\4305da1ea25c27fce08bd14001b76fd54fe42a0724bbd5168c76680a56eda5be-7b68544e47b1c778e3ede7ab68c7d8f26c2499c43fcd17b8fe6913fe153b2fdb.pkl\n",
      ".\\AppData\\Local\\Jedi\\Jedi\\CPython-313-33\\4305da1ea25c27fce08bd14001b76fd54fe42a0724bbd5168c76680a56eda5be-85f4d45dfd5e5c2122e3a6714dfdc2b6a6055ded69f22ac5d2958b604126eb60.pkl\n",
      ".\\AppData\\Local\\Jedi\\Jedi\\CPython-313-33\\4305da1ea25c27fce08bd14001b76fd54fe42a0724bbd5168c76680a56eda5be-899060205182bf831a9cd20a2a4f07b47f92ea0f222f087438dd5caf435083fa.pkl\n",
      ".\\AppData\\Local\\Jedi\\Jedi\\CPython-313-33\\4305da1ea25c27fce08bd14001b76fd54fe42a0724bbd5168c76680a56eda5be-b9e7185144bf64e42fb8abaa9d8a13fe894f04a66a10daf646aa085731ee1d8d.pkl\n",
      ".\\AppData\\Local\\Jedi\\Jedi\\CPython-313-33\\4305da1ea25c27fce08bd14001b76fd54fe42a0724bbd5168c76680a56eda5be-bf99683ae99694cf6e28391b7dc83a169120b142ef22b583ad79e98be8cfdc31.pkl\n",
      ".\\Desktop\\Data Science Full Stack\\Amdari\\Intenship\\Working directory\\artifacts\\feature_cols.pkl\n",
      ".\\Desktop\\Data Science Full Stack\\Amdari\\Intenship\\Working directory\\artifacts\\scaler.pkl\n",
      ".\\Desktop\\Data Science Full Stack\\Amdari\\Intenship\\Working directory\\artifacts\\xgb_model.pkl\n",
      ".\\Desktop\\Data Science Full Stack\\Python\\kmeans_encorder.pkl\n",
      ".\\Desktop\\Data Science Full Stack\\Python\\kmeans_model.pkl\n",
      ".\\Desktop\\Data Science Full Stack\\Python\\kmeans_scaler.pkl\n",
      ".\\Desktop\\Data Science Full Stack\\Python\\report.pkl\n",
      ".\\Desktop\\Data Science Full Stack\\Python\\rf_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for root, dirs, files in os.walk(\".\", topdown=True):\n",
    "    for file in files:\n",
    "        if file.endswith(\".pkl\"):\n",
    "            print(os.path.join(root, file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324cf262-8216-450a-87d4-75a852050dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now in: C:\\Users\\user\\Desktop\\Data Science Full Stack\\Amdari\\Intenship\\Working directory\n",
      "Artifacts contents: ['feature_cols.pkl', 'scaler.pkl', 'xgb_model.pkl']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(r\"C:\\Users\\user\\Desktop\\Data Science Full Stack\\Amdari\\Intenship\\Working directory\")\n",
    "\n",
    "print(\"Now in:\", os.getcwd())\n",
    "print(\"Artifacts contents:\", os.listdir(\"artifacts\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e52befe-9317-4b08-8ae7-3f775f9605a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded model, scaler and feature list successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "best_xgb = joblib.load(\"artifacts/xgb_model.pkl\")\n",
    "scaler = joblib.load(\"artifacts/scaler.pkl\")\n",
    "feature_cols = joblib.load(\"artifacts/feature_cols.pkl\")\n",
    "\n",
    "print(\"âœ… Loaded model, scaler and feature list successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008b4bbb-a3c5-45a9-a8fa-8f9e5b6b65fc",
   "metadata": {},
   "source": [
    "## Create FastAPI Inference Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9bb8d3-1cab-441a-8f80-310b3d53f2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "app = FastAPI(title=\"Stock Trend Prediction API\",\n",
    "              description=\"Predict Uptrend / Sideways / Downtrend using XGBoost model\",\n",
    "              version=\"1.0\")\n",
    "\n",
    "# ================= LOAD ARTIFACTS =====================\n",
    "model = joblib.load(\"artifacts/xgb_model.pkl\")\n",
    "scaler = joblib.load(\"artifacts/scaler.pkl\")\n",
    "feature_cols = joblib.load(\"artifacts/feature_cols.pkl\")\n",
    "\n",
    "# ================= HEALTH CHECK =======================\n",
    "@app.get(\"/\")\n",
    "def home():\n",
    "    return {\"status\": \"API is running ðŸš€\", \"features_required\": feature_cols}\n",
    "\n",
    "# ================= PREDICTION ENDPOINT =================\n",
    "@app.post(\"/predict\")\n",
    "def predict(data: dict):\n",
    "\n",
    "    # Convert input into DataFrame with correct order\n",
    "    df = pd.DataFrame([data])[feature_cols]\n",
    "\n",
    "    # scale input\n",
    "    X = scaler.transform(df)\n",
    "\n",
    "    # Predict class\n",
    "    pred = model.predict(X)[0]\n",
    "\n",
    "    label_map = {0: \"Downtrend ðŸ“‰\", 1: \"Sideways âž–\", 2: \"Uptrend ðŸ“ˆ\"}\n",
    "\n",
    "    return {\n",
    "        \"prediction_class\": int(pred),\n",
    "        \"trend_label\": label_map[pred],\n",
    "        \"status\": \"success\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588b90f8-567d-4139-be85-ad4770afddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting Final Matrix\n",
    "\n",
    "# Final evaluation numbers from your Week 2 notebook\n",
    "acc_best     = 0.3889   # classification accuracy\n",
    "total_return = 0.7423   # 74.23% strategy return\n",
    "win_rate     = 0.4064   # 40.64% winning trades\n",
    "max_drawdown = 0.9557   # 95.57% drawdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458715c1-9f5e-4803-9ec9-95b2db2404f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\utils.py:140: FutureWarning: Filesystem tracking backend (e.g., './mlruns') is deprecated. Please switch to a database backend (e.g., 'sqlite:///mlflow.db'). For feedback, see: https://github.com/mlflow/mlflow/issues/18534\n",
      "  return FileStore(store_uri, store_uri)\n",
      "2025/12/06 16:45:33 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh(<full-path-to-git-executable>)\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial message can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|silent|none|n|0: for no message or exception\n",
      "    - warn|w|warning|log|l|1: for a warning message (logging level CRITICAL, displayed by default)\n",
      "    - error|e|exception|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n",
      "2025/12/06 16:45:34 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/12/06 16:46:05 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MLflow run completed and model logged.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "\n",
    "# End any active run just in case\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "\n",
    "# Create/select experiment\n",
    "mlflow.set_experiment(\"stock_trend_classification\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"xgb_final_prod\"):\n",
    "    # 1) Hyperparameters\n",
    "    mlflow.log_params(best_xgb.get_params())\n",
    "\n",
    "    # 2) Metrics\n",
    "    mlflow.log_metric(\"accuracy\", acc_best)\n",
    "    mlflow.log_metric(\"total_return\", total_return)\n",
    "    mlflow.log_metric(\"win_rate\", win_rate)\n",
    "    mlflow.log_metric(\"max_drawdown\", max_drawdown)\n",
    "\n",
    "    # 3) Model artifact\n",
    "    mlflow.xgboost.log_model(best_xgb, artifact_path=\"xgb_model\")\n",
    "\n",
    "print(\"âœ… MLflow run completed and model logged.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a37370-3eee-481c-9a9d-9d4183063bcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['close',\n",
       " 'daily_return',\n",
       " 'sma_20',\n",
       " 'sma_50',\n",
       " 'ema_12',\n",
       " 'ema_26',\n",
       " 'rsi_14',\n",
       " 'macd',\n",
       " 'bb_width',\n",
       " 'atr_14',\n",
       " 'volume_ratio',\n",
       " 'momentum_10']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf70269-efb2-473f-b352-cc5322bdcff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating my FastAPI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415295dd-aac8-464d-a263-07a07420221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- Load artifacts ----------\n",
    "model = joblib.load(\"artifacts/xgb_model.pkl\")\n",
    "scaler = joblib.load(\"artifacts/scaler.pkl\")\n",
    "feature_cols = joblib.load(\"artifacts/feature_cols.pkl\")\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"Stock Trend Prediction API\",\n",
    "    description=\"Predict 0=Downtrend, 1=Sideways, 2=Uptrend from technical indicators\",\n",
    "    version=\"1.0.0\",\n",
    ")\n",
    "\n",
    "# ---------- Health check ----------\n",
    "@app.get(\"/\")\n",
    "def health():\n",
    "    return {\n",
    "        \"status\": \"ok\",\n",
    "        \"message\": \"API is running\",\n",
    "        \"required_features\": feature_cols,\n",
    "    }\n",
    "\n",
    "# ---------- Prediction endpoint ----------\n",
    "@app.post(\"/predict\")\n",
    "def predict(features: dict):\n",
    "    \"\"\"\n",
    "    Expects a JSON body with keys exactly matching feature_cols.\n",
    "    Example:\n",
    "    {\n",
    "      \"close\": 210.5,\n",
    "      \"sma_20\": 205.2,\n",
    "      ...\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert dict -> DataFrame in correct column order\n",
    "    df = pd.DataFrame([features])\n",
    "    df = df[feature_cols]              # ensure correct ordering\n",
    "\n",
    "    X_scaled = scaler.transform(df.values)\n",
    "    probs = model.predict_proba(X_scaled)[0]\n",
    "    pred_class = int(np.argmax(probs))\n",
    "\n",
    "    label_map = {0: \"Downtrend\", 1: \"Sideways\", 2: \"Uptrend\"}\n",
    "\n",
    "    return {\n",
    "        \"prediction_class\": pred_class,\n",
    "        \"trend_label\": label_map[pred_class],\n",
    "        \"probabilities\": {\n",
    "            \"class_0\": float(probs[0]),\n",
    "            \"class_1\": float(probs[1]),\n",
    "            \"class_2\": float(probs[2]),\n",
    "        },\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b4238d-789b-4d76-ac32-ff264f79d7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"status\": \"ok\", \"message\": \"Minimal API is running\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ef7c8a-03cf-493b-8472-ac248959bc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ================== LOAD MODEL + ARTIFACTS ==================\n",
    "model = joblib.load(\"artifacts/xgb_model.pkl\")\n",
    "scaler = joblib.load(\"artifacts/scaler.pkl\")\n",
    "feature_cols = joblib.load(\"artifacts/feature_cols.pkl\")\n",
    "\n",
    "# ================== CREATE API ==================\n",
    "app = FastAPI(\n",
    "    title=\"Stock Trend Prediction API\",\n",
    "    description=\"Predict market trend â†’ 0=Downtrend | 1=Sideways | 2=Uptrend\",\n",
    "    version=\"1.0\"\n",
    ")\n",
    "\n",
    "# ================== ROUTE: HEALTH CHECK ==================\n",
    "@app.get(\"/\")\n",
    "def home():\n",
    "    return {\n",
    "        \"status\": \"running\",\n",
    "        \"message\": \"Model API active\",\n",
    "        \"features_required\": feature_cols,\n",
    "        \"total_features\": len(feature_cols)\n",
    "    }\n",
    "\n",
    "# ================== ROUTE: PREDICTION ==================\n",
    "@app.post(\"/predict\")\n",
    "def predict(features: dict):\n",
    "\n",
    "    df = pd.DataFrame([features])  # Convert input dict â†’ DataFrame\n",
    "\n",
    "    try:\n",
    "        df = df[feature_cols]  # Reorder columns correctly\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"error\": \"Feature mismatch\",\n",
    "            \"details\": str(e),\n",
    "            \"expected_features\": feature_cols\n",
    "        }\n",
    "\n",
    "    X_scaled = scaler.transform(df.values)\n",
    "    probs = model.predict_proba(X_scaled)[0]\n",
    "    pred_class = int(np.argmax(probs))\n",
    "\n",
    "    trend_label = {0: \"Downtrend\", 1: \"Sideways\", 2: \"Uptrend\"}[pred_class]\n",
    "\n",
    "    return {\n",
    "        \"prediction_class\": pred_class,\n",
    "        \"trend_label\": trend_label,\n",
    "        \"probabilities\": {\n",
    "            \"Downtrend (0)\": float(probs[0]),\n",
    "            \"Sideways (1)\": float(probs[1]),\n",
    "            \"Uptrend (2)\": float(probs[2])\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939fe5c1-a661-40c0-8dd4-b90e8d36f50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "api.py created at: C:\\Users\\user\\Desktop\\Data Science Full Stack\\Amdari\\Intenship\\Working directory\\api.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from textwrap import dedent\n",
    "\n",
    "# 1) Make sure we're in your working directory\n",
    "os.chdir(r\"C:\\Users\\user\\Desktop\\Data Science Full Stack\\Amdari\\Intenship\\Working directory\")\n",
    "\n",
    "# 2) Minimal FastAPI app code\n",
    "api_code = dedent(\"\"\"\n",
    "    from fastapi import FastAPI\n",
    "\n",
    "    app = FastAPI()\n",
    "\n",
    "    @app.get(\"/\")\n",
    "    def root():\n",
    "        return {\"status\": \"ok\", \"message\": \"Minimal API is running\"}\n",
    "\"\"\")\n",
    "\n",
    "# 3) Write api.py file\n",
    "with open(\"api.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(api_code)\n",
    "\n",
    "print(\"api.py created at:\", os.path.abspath(\"api.py\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520b7a1a-5469-4dcc-9b06-8d5a4f624983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… api.py updated with model-serving API\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from textwrap import dedent\n",
    "\n",
    "# Make sure you're in the working directory\n",
    "os.chdir(r\"C:\\Users\\user\\Desktop\\Data Science Full Stack\\Amdari\\Intenship\\Working directory\")\n",
    "\n",
    "api_code = dedent(\"\"\"\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from fastapi import FastAPI\n",
    "\n",
    "    # ===== Load Artifacts =====\n",
    "    model = joblib.load(\"artifacts/xgb_model.pkl\")\n",
    "    scaler = joblib.load(\"artifacts/scaler.pkl\")\n",
    "    feature_cols = joblib.load(\"artifacts/feature_cols.pkl\")\n",
    "\n",
    "    app = FastAPI(\n",
    "        title=\"Stock Trend Prediction API\",\n",
    "        description=\"Predict 0=Downtrend, 1=Sideways, 2=Uptrend using the tuned XGBoost model\",\n",
    "        version=\"1.0.0\",\n",
    "    )\n",
    "\n",
    "    @app.get(\"/\")\n",
    "    def home():\n",
    "        return {\n",
    "            \"status\": \"running\",\n",
    "            \"message\": \"Model API active\",\n",
    "            \"n_features\": len(feature_cols),\n",
    "            \"features_required\": feature_cols,\n",
    "        }\n",
    "\n",
    "    @app.post(\"/predict\")\n",
    "    def predict(features: dict):\n",
    "        # Convert input dict to DataFrame\n",
    "        df = pd.DataFrame([features])\n",
    "\n",
    "        # Reorder columns to match training\n",
    "        try:\n",
    "            df = df[feature_cols]\n",
    "        except KeyError as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"message\": f\"Feature mismatch: {e}\",\n",
    "                \"expected_features\": feature_cols,\n",
    "            }\n",
    "\n",
    "        # Scale inputs\n",
    "        X_scaled = scaler.transform(df.values)\n",
    "\n",
    "        # Predict probabilities\n",
    "        probs = model.predict_proba(X_scaled)[0]\n",
    "        pred_class = int(np.argmax(probs))\n",
    "\n",
    "        label_map = {0: \"Downtrend\", 1: \"Sideways\", 2: \"Uptrend\"}\n",
    "\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"prediction_class\": pred_class,\n",
    "            \"trend_label\": label_map[pred_class],\n",
    "            \"probabilities\": {\n",
    "                \"class_0\": float(probs[0]),\n",
    "                \"class_1\": float(probs[1]),\n",
    "                \"class_2\": float(probs[2]),\n",
    "            },\n",
    "        }\n",
    "\"\"\")\n",
    "\n",
    "with open(\"api.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(api_code)\n",
    "\n",
    "print(\"âœ… api.py updated with model-serving API\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5527d3ec-ec8c-4485-82e7-a1ce5cc431a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['close',\n",
       " 'daily_return',\n",
       " 'sma_20',\n",
       " 'sma_50',\n",
       " 'ema_12',\n",
       " 'ema_26',\n",
       " 'rsi_14',\n",
       " 'macd',\n",
       " 'bb_width',\n",
       " 'atr_14',\n",
       " 'volume_ratio',\n",
       " 'momentum_10']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib, os\n",
    "os.chdir(r\"C:\\Users\\user\\Desktop\\Data Science Full Stack\\Amdari\\Intenship\\Working directory\")\n",
    "feature_cols = joblib.load(\"artifacts/feature_cols.pkl\")\n",
    "feature_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57776a3d-2164-422e-8c4f-bef1d0c9f09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c312d035-073a-45b9-93fd-ec89f976ac6f",
   "metadata": {},
   "source": [
    "# Updating the log predictions for monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef044ea-2335-4b22-84ce-d1853f0d1a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… predict() updated with logging\n"
     ]
    }
   ],
   "source": [
    "import os, textwrap\n",
    "\n",
    "os.chdir(r\"C:\\Users\\user\\Desktop\\Data Science Full Stack\\Amdari\\Intenship\\Working directory\")\n",
    "\n",
    "# Read the current api.py\n",
    "with open(\"api.py\", \"r\", encoding=\"utf-8\") as f:\n",
    "    api_text = f.read()\n",
    "\n",
    "new_predict = textwrap.dedent(\"\"\"\n",
    "    @app.post(\"/predict\")\n",
    "    def predict(features: dict):\n",
    "        import csv\n",
    "        from datetime import datetime\n",
    "        import os\n",
    "\n",
    "        # Convert input dict to DataFrame\n",
    "        df = pd.DataFrame([features])\n",
    "\n",
    "        # Reorder columns to match training\n",
    "        try:\n",
    "            df = df[feature_cols]\n",
    "        except KeyError as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"message\": f\"Feature mismatch: {e}\",\n",
    "                \"expected_features\": feature_cols,\n",
    "            }\n",
    "\n",
    "        # Scale inputs\n",
    "        X_scaled = scaler.transform(df.values)\n",
    "\n",
    "        # Predict probabilities\n",
    "        probs = model.predict_proba(X_scaled)[0]\n",
    "        pred_class = int(np.argmax(probs))\n",
    "\n",
    "        label_map = {0: \"Downtrend\", 1: \"Sideways\", 2: \"Uptrend\"}\n",
    "        trend_label = label_map[pred_class]\n",
    "\n",
    "        # ===== Simple logging to CSV for monitoring =====\n",
    "        os.makedirs(\"logs\", exist_ok=True)\n",
    "        log_file = os.path.join(\"logs\", \"predictions_log.csv\")\n",
    "        write_header = not os.path.exists(log_file)\n",
    "\n",
    "        row = {\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            \"prediction_class\": pred_class,\n",
    "            \"trend_label\": trend_label,\n",
    "            \"prob_class_0\": float(probs[0]),\n",
    "            \"prob_class_1\": float(probs[1]),\n",
    "            \"prob_class_2\": float(probs[2]),\n",
    "        }\n",
    "        # add input features to the row\n",
    "        for k, v in features.items():\n",
    "            row[f\"feat_{k}\"] = v\n",
    "\n",
    "        with open(log_file, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=row.keys())\n",
    "            if write_header:\n",
    "                writer.writeheader()\n",
    "            writer.writerow(row)\n",
    "        # ================================================\n",
    "\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"prediction_class\": pred_class,\n",
    "            \"trend_label\": trend_label,\n",
    "            \"probabilities\": {\n",
    "                \"class_0\": float(probs[0]),\n",
    "                \"class_1\": float(probs[1]),\n",
    "                \"class_2\": float(probs[2]),\n",
    "            },\n",
    "        }\n",
    "\"\"\")\n",
    "\n",
    "# Replace the old predict function in api.py\n",
    "import re\n",
    "api_text = re.sub(r\"@app.post\\(\\\"/predict\\\"[\\s\\S]+?return \\{[\\s\\S]+?\\}\\s*\\n\", new_predict + \"\\n\", api_text)\n",
    "\n",
    "with open(\"api.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(api_text)\n",
    "\n",
    "print(\"âœ… predict() updated with logging\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5af83ae-cf9e-4402-a486-c51a6f12fbd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ade67ef",
   "metadata": {},
   "source": [
    "## Getting the Docker for the project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f80d5ff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c608cdc2-821d-41fb-8e12-9308e6239bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from textwrap import dedent\n",
    "\n",
    "os.chdir(r\"C:\\Users\\user\\Desktop\\Data Science Full Stack\\Amdari\\Intenship\\Working directory\")\n",
    "\n",
    "dockerfile = dedent(\"\"\"\n",
    "    FROM python:3.11-slim\n",
    "\n",
    "    WORKDIR /app\n",
    "\n",
    "    COPY requirements.txt .\n",
    "\n",
    "    RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "    COPY . .\n",
    "\n",
    "    EXPOSE 8000\n",
    "\n",
    "    CMD [\"uvicorn\", \"api:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "\"\"\")\n",
    "\n",
    "with open(\"Dockerfile\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(dockerfile)\n",
    "\n",
    "print(\"âœ… Dockerfile created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99baa5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reqs = \"\"\"\n",
    "fastapi\n",
    "uvicorn[standard]\n",
    "pandas\n",
    "numpy\n",
    "xgboost\n",
    "scikit-learn\n",
    "joblib\n",
    "mlflow\n",
    "\"\"\"\n",
    "with open(\"requirements.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(reqs.strip() + \"\\n\")\n",
    "\n",
    "print(\"âœ… requirements.txt created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f8cf49",
   "metadata": {},
   "source": [
    "## Interactive Dashboard with Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1508ac7-314d-4395-b861-51511380d4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from textwrap import dedent\n",
    "\n",
    "os.chdir(r\"C:\\Users\\user\\Desktop\\Data Science Full Stack\\Amdari\\Intenship\\Working directory\")\n",
    "\n",
    "streamlit_code = dedent(\"\"\"\n",
    "    import streamlit as st\n",
    "    import requests\n",
    "\n",
    "    st.set_page_config(page_title=\"Stock Trend Dashboard\", layout=\"centered\")\n",
    "\n",
    "    st.title(\"ðŸ“ˆ Stock Trend Prediction Dashboard\")\n",
    "    st.write(\"This dashboard sends features to the FastAPI model and shows the predicted trend.\")\n",
    "\n",
    "    # Sidebar for API config\n",
    "    st.sidebar.header(\"API Configuration\")\n",
    "    api_url = st.sidebar.text_input(\"FastAPI URL\", \"http://127.0.0.1:8000/predict\")\n",
    "\n",
    "    st.subheader(\"Input Features\")\n",
    "\n",
    "    close = st.number_input(\"Close price\", value=210.5)\n",
    "    daily_return = st.number_input(\"Daily return\", value=0.003, format=\"%.5f\")\n",
    "    sma_20 = st.number_input(\"SMA 20\", value=208.1)\n",
    "    sma_50 = st.number_input(\"SMA 50\", value=200.4)\n",
    "    ema_12 = st.number_input(\"EMA 12\", value=207.9)\n",
    "    ema_26 = st.number_input(\"EMA 26\", value=203.2)\n",
    "    rsi_14 = st.number_input(\"RSI 14\", value=55.3)\n",
    "    macd = st.number_input(\"MACD\", value=0.42)\n",
    "    bb_width = st.number_input(\"Bollinger Band Width\", value=0.05, format=\"%.4f\")\n",
    "    atr_14 = st.number_input(\"ATR 14\", value=2.1)\n",
    "    volume_ratio = st.number_input(\"Volume ratio\", value=1.2)\n",
    "    momentum_10 = st.number_input(\"Momentum 10\", value=1.5)\n",
    "\n",
    "    if st.button(\"Predict Trend\"):\n",
    "        payload = {\n",
    "            \"close\": close,\n",
    "            \"daily_return\": daily_return,\n",
    "            \"sma_20\": sma_20,\n",
    "            \"sma_50\": sma_50,\n",
    "            \"ema_12\": ema_12,\n",
    "            \"ema_26\": ema_26,\n",
    "            \"rsi_14\": rsi_14,\n",
    "            \"macd\": macd,\n",
    "            \"bb_width\": bb_width,\n",
    "            \"atr_14\": atr_14,\n",
    "            \"volume_ratio\": volume_ratio,\n",
    "            \"momentum_10\": momentum_10,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            res = requests.post(api_url, json=payload, timeout=10)\n",
    "            if res.status_code == 200:\n",
    "                data = res.json()\n",
    "                st.success(f\"Predicted trend: **{data['trend_label']}** (class {data['prediction_class']})\")\n",
    "                st.write(\"Probabilities:\")\n",
    "                st.json(data[\"probabilities\"])\n",
    "            else:\n",
    "                st.error(f\"API returned status code {res.status_code}\")\n",
    "                st.text(res.text)\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error calling API: {e}\")\n",
    "\"\"\")\n",
    "\n",
    "with open(\"dashboard_app.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(streamlit_code)\n",
    "\n",
    "print(\"âœ… dashboard_app.py created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5f2ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 1. Go to your working directory\n",
    "os.chdir(r\"C:\\Users\\user\\Desktop\\Data Science Full Stack\\Amdari\\Intenship\\Working directory\")\n",
    "\n",
    "# 2. Show what's in there\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(os.listdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dec651c-32c7-43ad-9810-845a8531b2ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8343b755-cf2b-4f71-bf8e-33303bbd86fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1701a44d-9e60-47c5-bff5-64b42dfa1132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
